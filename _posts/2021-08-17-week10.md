---
layout: post
title: "[Week 10] time for perfecting the latency estimator (3)"
author: Jiyeon Lee
categories: GSOC
tags: [GSOC, make_database]
image: week10.jpeg
---

## Get real model's params for getting regressor score

Model used : densenet, MobileNet from [tflite model zoo](https://www.tensorflow.org/lite/guide/hosted_models?hl=ko), and [hair segmentation](https://github.com/google/mediapipe/blob/master/mediapipe/models/hair_segmentation.tflite), [face detection](https://github.com/google/mediapipe/blob/master/mediapipe/modules/face_detection/face_detection_short_range.tflite) from mediapipe github

total : 245 samples

![img](assets/img/week10/img1.png)

<br/>

But it seems worse... 

![img](assets/img/week10/img2.png)

I think it would be better to include some of benchamrks of the real model in the train set. Not all, but adding about 20% would be good. 

<br/><br/>

## Analyse Regressors

### What is XGBoost?

> eXtreame Gradient Boosting

It is an implementation of gradient boosting machines. It is a decision-tree based ensemble Machine learning algorithm that uses a gradient boosting framework. When it comes to small-to-medium structured/tabular data, decision tree based algorithm are considered best-in-class right now. 

<https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d>